#!/bin/bash
#! Make sure all the SBATCH directives go in this section and no other commands
#! Name of the job:
#SBATCH -J asymp_scan
#! Project name for Cunniffe group, use SL2 for paying queue:
#SBATCH -A CUNNIFFE-SL3-CPU
#! Output filename:
#! %A means slurm job ID and %a means array index
#SBATCH --output=%A_%a.out 
#! Errors filename:
#SBATCH --error=%A_%a.err 
#! How many whole nodes should be allocated? (for single core jobs always leave this at 1)
#SBATCH --nodes=1
#! How many tasks will there be in total? (for single core jobs always leave this at 1)
#SBATCH --ntasks=1
#! How many many cores will be allocated per task? (for single core jobs always leave this at 1)
#SBATCH --cpus-per-task=1 
#! Estimated runtime: hh:mm:ss (job is force-stopped after if exceeded):
#! less than 12 hours on SL3 (non-paid)
#SBATCH --time=6:59:00
#! Estimated maximum memory needed (job is force-stopped if exceeded):
#! Never request less than 5980mb of memory. 
#! RAM is allocated in ~5980mb blocks, you are charged per block used, 
#! and unused fractions of blocks will not be usable by others.
#SBATCH --mem=3380mb
#! Submit a job array with index values
#! NOTE: This must be a range, not a single number (i.e. specifying '32' here would only run one job, with index 32). Here top number is x=n_p-1 (0-x is then n_p iterations)
#SBATCH --array=0-99

#! This is the partition name. This will request for a node with 6GB RAM for each task
#SBATCH -p skylake,cclake,icelake


# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

#! Don't put any #SBATCH directives below this line, it is now safe to put normal commands below this line

#! Modify the environment seen by the application. For this example we need the default modules.
. /etc/profile.d/modules.sh                # This line enables the module command
module purge                               # Removes all modules still loaded
module load rhel7/default-peta4            # REQUIRED - loads the basic environment
module load use.own                        # This line loads the own module list
module load /rds/project/cag1/rds-cag1-general/epidem-modules/epidem.modules         # Loads Epidemiology group module list
module load miniconda3/4.9.2

# Conda set up
# >>> conda initialize >>>
# Contents within this block are managed by 'conda init' !!
__conda_setup="$('/rds/project/cag1/rds-cag1-general/epidem-programs/miniconda3/4.9.2/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/rds/project/cag1/rds-cag1-general/epidem-programs/miniconda3/4.9.2/etc/profile.d/conda.sh" ]; then
        . "/rds/project/cag1/rds-cag1-general/epidem-programs/miniconda3/4.9.2/etc/profile.d/conda.sh"
    else
        export PATH="/rds/project/cag1/rds-cag1-general/epidem-programs/miniconda3/4.9.2/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda initialize <<<

conda activate poly2_env

#! Command line that we want to run:
#! The variable $SLURM_ARRAY_TASK_ID contains the array index for each job.
#! In this example, each job will be passed its index

python -m cluster.scan_asymp $SLURM_ARRAY_TASK_ID
